02.10.2025
2212HRS

We begin with data acquisition. The intent is to use real-world data and show true, and possibly updatable analytics.

Here's what I'm thinking:
let's use public retail datasets. it should be possible to scrape their data directly and update it automatically. I'll link my postgres to my vscode and run my scripts on my pgadmin. you can teach me how to make that work. i think we can work it such that the scraped data is updated regularly from their site and the analytics are updated automatically. how can we go about that? is it workable?

but i looked it up now and found out that no companies have their retail data publicly available. That being the case, let's use the static data like that, call it directly like it's in a database to simulate caling from a live updatable database, and analyze it, then create the dashboard to show the results of the analyses. then add the code that periodically updates itself from the data source.

This way, we can productize the project such that anyone can link it up to their database and have it analyzed.

That should be possible yeah?

It should be. I'm thinking of using the SUPERSTORE dataset.

So why not just take in the large superstore dataset and split it into different tables all linked to each other in the appropriate way? these tables will form the database and make it easier for the company clerk to input data going forward. We don't need to simulate anything we don't have. It's unnecessary in my opinion.

analyze the data using postgres and then present the results using power bi.

the refresh would then be at the dataset loading section while the rest of the process from splitting the larger table into the different tables is automated. this automatically updates the results of our queries and thereby, updates the dashboard

